## NL INTELL: From Command Line Exploration To Bandwidth-Enabled Embarrassingly Parallel Computing

Python was not originally *supposed* to be for high performance computing although it has found its way into that realm because it made comptation accessible, friendly and just generally FUN ... you might claim that the Python was always destined to be accessible because of its roots in the ABC language ... but the push to make Python faster, object oriented and *well-behaved* as more processor-friendly language has also helped keep the language from being merely a pedagogical language ... it is the battles inside Python dev community that helped Python to become even more Pythonic, ie it is the people CARE about how they use *their* languange who make ***their*** langauge better ... it's similar perhaps to how the vulgar tongue of the barbarian English became increasingly *English* and possibly more chaotic and worldly and less insular and teutonic over a thousand years -- it was the *USE* of langauge in global trade by the unwashed masses that made English more *usable* ... it's about the people who *use* the language and therefore *CARE* more how it is used ... in English, it was the battling between the vulgar corruptions of uncivilized *USERS* and the attempts to refine, purify and civilize Oxford English language. The BATTLE is what matters; but for there to be battle, someone has to have reason to care. ***In the final analysis, it is the competing forces and antagonistic BATTLES inside a language from people who push things too far and offend other users with their uncouth uses that make the language useful.*** 

It is extremely important to examine what Python is for ... this is NOT *entirely* a matter of looking at original intent, which does matter but it is important to examine how Python is being used, how people want to use it and push it to be used. Python will continue being heavily used with C, Rust, hardware acceleration and all kinds of tactics to get even more mileage out of Python ... but the architecture of models and algorithms and use will also help to refine and extend the language ... but then again, when Python stops being useful, something else will take its place. 

***In general, it is extremely important to think about LANGUAGE as tool for THE CROWD to efficiently accomplish a purpose ... in the same way, that books rapidly became an efficient way to educate or share knowledge.***  We should occasionally think about why/how we rely upon language ... it's like plumbing, electricity or HVAC systems -- exactly WHY do we rely upon these utilitarian things in the manner that we do ... what are trying to accomplish? Is there a better way of doing it. WHY did that utility become such a common standard that nobody questions it? Can we bypass it or do it better with something else now.

*Knowledge and information might be more important than water, light, heat, AC ... just because how knowledge and information shape how we use ALL other tools.*

CROWDS need languages. Individuals, like individual animals who only use ephemeral signalling, do not really need that much of a language *IMMEDIATELY* ... but they will need that language when they need something the crowd provides ... think about going to foreign country and, maybe being injured or hurt NEEDING medical care ... or similarly helpless and NEEDING to use a restroom, or NEEDING a drink of water or something to eat ... indiviuals need to use the crowd's language to get minimal access to what kinds of things that the crowd can provide. At the pinnacle of crowd leadership, the individual needs to be able to speak in the language of power ... we might not NEED or even want much power, *especially after we understand what kinds of nastiness that centers of power attract ...* but, even the court jester needs to be able to speak in the language of power to avoid immediate execution. That which we cannot speak of, must quote Wittgenstein or just shut up -- in other words, we probably want to at least be able to hack our way through the basics in those languages that helps us get or process knowledge and information. 

In humans, communication was probably at first pretty much ephemeral, only something gestured including the sounds was uttered or spoken ... but then it became richer or carried more meaning, perhaps poetically or rhythmically with a dance, but in ways that stuck in memory ... then the day arrived, when language could be made more permanent to be painted, carved, then written -- then in in better ways that could be better passed down read by others transcending time to give the language users an advantage ... the innovations in language continued with knowledge that could be printed with carved blocks for dessimination and coordination of policy, then knowledge could typeset, printed mechanically and language got out of control of the language keepers and scribes ... which radically spread the audience and made it possible for others to have careers in things that were about knowledge and ideas ...  then language was distributed as a signal or code down telegraph lines for immediacy betweeen points ... then something that could be carried over a radio waves for widespread, mass communication immediacy ... then multiplexing and computational code division multiple access along with the other improvements, eg. 4G, 5G and everything for MUCH, MUCH, MUCH higher comms bandwidth and accelerating the internetworking of the CROWD ... without our ability to prcesss a CDMA signal, we're kinda at a disadvantage to those in the  crowd with ability to use the high bandwidth signal the crowd is using ... and further boosting, to involve BILLIONS rather than a few million in the competitive rate of knowledge engineering and more rapid understanding. 

# The CROWD of langauage users instantaeously blows away what any single individual can accomplish in a lifetime. 

This is not particularly new ... but egotistical humans USED to be able to imagine that what they alone could pull off mattered.  NOT ANY MORE ... although the ignorance of egomaniacs can persist indefinitely. But humans exchanging information is not new ... efficiently disseminating, analyzing and optimizing every aspect of the value of information is the *killer app* of the human species. What is *slightly* different now is the SCALE of the crowd ... it's tougher ignore now that one's own accomplishments, one's lineable, one's family, one's little town DO NOT MATTER THAT MUCH in comparison to the BILLIONS who are USING the langauge of information transfer more effectively now than they did last year, much more effectively than they did five years ago, ridiculously more effectively than they did ten years ago. 

The rise of Python is part of that. Python does nothing to change how humans use whatever works to get at the goal of using information more efficiently and expeditiously. Python enables the conversion of data into knowledge by addressing the HUMAN constraints in understanding computerese ... thus changes the game in data-to-knowledge conversion ... data have always existed, look at the data streaming in human-built sensors now, traveling at the speed of light but from the origins of Universe telling what happened billions of years ago when that data as radiation started it journey ... but the data humans care about do not matter for their own sake; SOMEBODY needs to ***USE*** those data, or the lifecyle of data is pretty short. Unused data get wiped out, deleted, trashed, maybe not immediately, but soon enough. This is the lesson of information, language, knowledge ... it's the USERS, look at who *USES* it ... otherwise, *NOBODY* is going to care.

This means that Python MIGHT BE well on its way to becoming the *new English* or most important language for scientists, engineers and those who work in the realm of data and transforming data into knowledge ... pre-trained models with data are now *beginning to* drive how technical information is shared and used ... even though, AI/ML is still in the very early stages, ie like the printing presses were when humans were still trying to imagine how the mechanically-printed word would change anything from expert-copied scrolls and texts. *What is language for? How does a compiler work ... or ... why do we use compilers?*

Early on, Python ***was***, in the first decade of two of its use, as a vastly improved derivative of the ABC educational programming language, a way for non-programmers, or at least non-C language and non-AssemblyLanguage programmers to become more systematic, reproducible, more efficiently programmatic in their problem-solving, ie early Python was known for being devastatingly effective as a more interactive, interwebs-connected replacement for a desktop calculator. While many have been focused upon things like [PyTorch](https://pytorch.org/docs/stable/index.html) or [TensorFlow](https://www.tensorflow.org/learn) or the [HuggingFace Hub](https://huggingface.co/docs/hub/main/en/index), there are plenty of big developments in more fundamental things like the [new NumPy](https://numpy.org/news/) or even the standard library in Python, eg the ***new*** Python's not really all THAT slow any more, at least it's not as slow as it was way back in 2022 before [the stable release of the faster Python **3.11**](https://www.python.org/downloads/release/python-3110/) started being used.

But Python is still a very, very, very high level language ... so after [thirty years of distillation](https://learning.oreilly.com/library/view/python-distilled/9780134173399/ch01.xhtml) it is still about the basics of Python for non-programmers and using Python interactively with iPython or a Jupyter Notebook interface, often collaboratively or allowing other nonprogrammers to *play with the code* ... it's the ***collaborability*** and ubiquitous, wider array of alternatives to use the popular language which makes Python [as popular as it is NOW](https://www.tiobe.com/tiobe-index/) ... and, it is likely, that since the lower-level parts of the Python ... or ***Python***-*As-Collaborative-Interwebs-Connected-Desk-Calculator* ... which are implemented in C ... which drives part of the rising popularity of C ... who knows exactly how much of C's recurrent popularity might be explained by low-level optimizations of code for Python libraries ... but C and Python should be seen as complements, not substitutes. Both are NECESSARY for what their very, very different levels.  

High level stuff matters, maybe even more than the low level stuff ... but the high level depends upon the low level. *And vice versa.* For example, most natural language insights will come out because the audience with resources first used a very, very, very high-level exploratory language to understand the problem space and marshall resources ... so, it makes sense to become fluent in command line Python and adopting a [Pythonic mindset in classes, objects, methods, functions and libraries](https://learning.oreilly.com/library/view/fluent-python-2nd/9781492056348/ch11.html) ... because the Python-As-Collaborative-Desk-Calculator model is how we might want to think of Python and it's power users who are *normal* mainstream types, rather than programmers ... it's important that Python is now such a ubiquitously-used language; it's not for the hard core programmers [who opt for C or Rust] as much as it is for power users. Python is the new lingua franca of science/tech, the new English, the new Excel spreadsheet.

Yes, at some point ... rather VERY quickly perhaps, because of how Python-for-power-users can help accelerate a general understanding of the problem's domain space ... many NLP tasks evolve into something far too computationally intensive for pure Python implementations to be feasible ... it's time for low-level tensor optimizations of problem with C language, GPUs and AI accelerators. However, in some cases the expense only arises when training models or tokenizing, not when using NLTK to label inputs ... so it's possible to use things like "fast" tokenizers OR find other ways to specifically attack the constraint which is the current barrier to more rapid execution, ie *draining the pond exposes news rocks under the boat which can be removed with the pond level  lower.* ALL parts of the workflow can be optimized, eg NLTK's package system provides a convenient way to distribute trained models but there ways to add a Git workflow OR container registry to make the transition rapid from the Pythonic command line interfaces to high-performance computing and parallel programming machine learning approaches like MapReduce and things that take us past MapReduce.

**Basic linguistic or semiotic analysis matters MUCH MORE with GPU computing grids, powerful ML accelerators and new frontiers in BigCompute ... having the computational power to compute more teraflops actually prevents many from ever achieving insights.**

Before we get too carried away with how much more can be accomplished today than a year ago with GPUs and better memory bandwidth architectures, it is best to keep focused the most FUNDAMENTAL of most FUNDAMENTAL problems. The entire toolchain matters ... including how we initially collect and use data APIs,  wrangle the data and perform the most basic NLP computational assignments ... the ENTIRE systematic approach of the Natural Language ToolKit (NLTK) in Python will continue to be relevant, even as our methods evolve and we develop or improve upon those method.

**In order to efficiently, effectively and rapidly scale, it will always be imperative to understand the fundamentals better ... or else, we end up automating a complete shitstorm which is even more impossible to begin to clean up because of the magic of automation.**

Thus we start off by revisiting [O'Reilly's collection on Natural Language Processing](https://learning.oreilly.com/topics/natural-language-processing/) and [Jon Krohn](https://github.com/jonkrohn)'s [Expert Playlist on Machine Learning](https://learning.oreilly.com/playlists/a40ea8fe-994d-4370-8b29-0d6c0f519a89/) ... this can rapidly take us into the overwhelming task of attempting to stay somewhat current reading and trying out hundreds of different proven Transformer and Diffuser models [and variations on those models] that we can find on Hugging Face Hub and in other corners of the interwebs ... as we understand, we can automate and improve the lower level execution, but it still helps us if we think of a general roadmap for studying semiotics and computational linguistics to better understand cognitive neuroscience of human crowds.  

## Phonology and Morphology
*How does the CROWD process information?* OR *How can we better achieve our objectives in knowledge engineering by using the constantly-learning crowd to more effectively process information?* We start by understanding the fundmentals of very fundamental stuff ... getting to very first principles of what underlies our assumptions when we analyze things from a *ab initio* or first principles approach. Computational approaches to the study of sound patterns and word structures typically use a finite state toolkit. Phenomena such as suppletion and non-concatenative morphology are difficult to address using the string processing methods we have been studying. The technical challenge is not only to link NLTK to a high-performance finite state toolkit, but to avoid duplication of lexical data and to link the morphosyntactic features needed by morph analyzers and syntactic parsers.

## Lexical Semantics
This is a vibrant area of current research, encompassing inheritance models of the lexicon, ontologies, multiword expressions, etc, mostly outside the scope of NLTK as it stands. A conservative goal would be to access lexical information from rich external stores in support of tasks in word sense disambiguation, parsing, and semantic interpretation.

## Natural Language Generation
Producing coherent text from underlying representations of meaning is an important part of NLP; a unification based approach to NLG has been developed in NLTK, and there is scope for more contributions in this area.

## Linguistic Fieldwork
A major challenge faced by linguists is to document thousands of endangered languages, work which generates heterogeneous and rapidly evolving data in large quantities. More fieldwork data formats, including interlinear text formats and lexicon interchange formats, could be supported in NLTK, helping linguists to curate and analyze this data, while liberating them to spend as much time as possible on data elicitation.

## Other Languages
Improved support for NLP in languages other than English could involve work in two areas: obtaining permission to distribute more corpora with NLTK's data collection; writing language-specific HOWTOs for posting at http://nltk.org/howto, illustrating the use of NLTK and discussing language-specific problems for NLP including character encodings, word segmentation, and morphology. NLP researchers with expertise in a particular language could arrange to translate this book and host a copy on the NLTK website; this would go beyond translating the discussions to providing equivalent worked examples using data in the target language, a non-trivial undertaking.

## The Larger Python Dev Community, including NLTK, Sphinx and 500,000 other libraries
Study different Dev communities and you will find a similar pattern. For example, consider how many people depend upon Sphinx and the ReadTheDocs documentation sites. As another example, many of NLTK's core components were contributed *without any compensation* by members of the NLP scientific community.  These components were housed in NLTK's "Contrib" package, nltk_contrib, with the only requirement for contributed software to be considered for addition was that the package must be written in Python, relevant in some way to the field of Natural Language Processing, and given the same open source license as the rest of NLTK. Imperfect software was/is welcome, actually even seen as NECESSARY ... it has to start somewhere ... the evolving discussion, in the issues, becomes the basis of friendships and professional relationships which will develop and improve over time ... the resulting open source software might be taken for granted by most users, but the development of these important portion of these libraries will continue to be the basis of trusted relationships by different members of the open source community ... you might not be paid right away for your contribution, but *you will get more out of these efforts than what you put in*.

## Teaching Materials
Since the earliest days of NLTK development, teaching materials have accompanied the software, materials that have gradually expanded to fill this book, plus a substantial quantity of online materials as well. We hope that instructors who supplement these materials with presentation slides, problem sets, solution sets, and more detailed treatments of the topics we have covered, will make them available, and will notify the authors so we can link them from http://nltk.org/. Of particular value are materials that help NLP become a mainstream course in the undergraduate programs of computer science and linguistics departments, or that make NLP accessible at the secondary level where there is significant scope for including computational content in the language, literature, computer science, and information technology curricula.


## Open Source Basics and The Python Development Community
1. Language Processing, Document Utilities and Python
2. Accessing Text Corpora, Wrangling Lexical Resources
3. Processing Raw Semiotics, At The Edge
4. Structured Programs, Structures of Structures, Managing MLops Workflow
5. Categorizing and Tagging Words, Semiotics, Images, Sounds, Smells
6. Learning to Classify, Tokenization
7. Vectorization, Extracting Basic Information
8. Analyzing Structure, Context, Meaning
9. Improving Feature-Based Models, Grammars, Idioms
10. Analyzing Context, Double Entendre, Misunderstandings 
11. Managing Linguistic Data and MLops Pipelines
12. The Rapidly Evolving Model Space 
